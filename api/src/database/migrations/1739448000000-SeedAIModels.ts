import { MigrationInterface, QueryRunner } from 'typeorm';

export class SeedAIModels1739448000000 implements MigrationInterface {
  public async up(queryRunner: QueryRunner): Promise<void> {
    // First, alter table to add new columns
    await queryRunner.query(`
      ALTER TABLE "ai_models"
      ADD COLUMN IF NOT EXISTS "latency_ms" DECIMAL(10,2),
      ADD COLUMN IF NOT EXISTS "throughput_tps" INTEGER,
      ADD COLUMN IF NOT EXISTS "cost_per_1m_input" DECIMAL(10,4),
      ADD COLUMN IF NOT EXISTS "cost_per_1m_output" DECIMAL(10,4),
      ADD COLUMN IF NOT EXISTS "cache_read_cost_per_1m" DECIMAL(10,4),
      ADD COLUMN IF NOT EXISTS "cache_write_cost_per_1m" DECIMAL(10,4),
      ADD COLUMN IF NOT EXISTS "image_gen_cost" DECIMAL(10,4),
      ADD COLUMN IF NOT EXISTS "video_gen_cost" DECIMAL(10,4),
      ADD COLUMN IF NOT EXISTS "web_search_cost" DECIMAL(10,4),
      ADD COLUMN IF NOT EXISTS "supports_image_gen" BOOLEAN DEFAULT false,
      ADD COLUMN IF NOT EXISTS "supports_video_gen" BOOLEAN DEFAULT false,
      ADD COLUMN IF NOT EXISTS "supports_web_search" BOOLEAN DEFAULT false,
      ADD COLUMN IF NOT EXISTS "available_providers" TEXT[] DEFAULT '{}';
    `);

    // Remove old cost columns if they exist
    await queryRunner.query(`
      ALTER TABLE "ai_models"
      DROP COLUMN IF EXISTS "cost_per_1k_input",
      DROP COLUMN IF EXISTS "cost_per_1k_output",
      DROP COLUMN IF EXISTS "average_speed_ms";
    `);

    // Seed models from Vercel AI Gateway
    const models = [
      // ========== TEXT GENERATION MODELS ==========
      
      // OpenAI Models (Most Popular)
      {
        modelId: 'openai:gpt-4o',
        modelName: 'GPT-4o',
        provider: 'openai',
        version: '4o',
        contextWindow: 128000,
        maxTokens: 16384,
        latencyMs: 600,
        throughputTps: 137,
        costPer1mInput: 1.25,
        costPer1mOutput: 10.00,
        cacheReadCostPer1m: 0.13,
        costBucket: 'medium',
        capabilities: ['text', 'vision', 'json', 'streaming', 'function_calling'],
        supportsVision: true,
        supportsStreaming: true,
        supportsJsonMode: true,
        supportsFunctionCalling: true,
        supportsImageGen: true,
        supportsWebSearch: true,
        imageGenCost: 10.00,
        webSearchCost: 10.00,
        availableProviders: ['openai', 'azure'],
        isRecommended: true,
        priorityRank: 1,
        useCases: ['content_ideas', 'complex_reasoning', 'vision_analysis'],
        description: 'Best balance of speed, cost, and quality for most tasks',
      },
      {
        modelId: 'openai:gpt-4o-mini',
        modelName: 'GPT-4o Mini',
        provider: 'openai',
        version: '4o-mini',
        contextWindow: 128000,
        maxTokens: 16384,
        latencyMs: 400,
        throughputTps: 205,
        costPer1mInput: 0.15,
        costPer1mOutput: 0.60,
        cacheReadCostPer1m: 0.07,
        costBucket: 'low',
        capabilities: ['text', 'vision', 'json', 'streaming'],
        supportsVision: true,
        supportsStreaming: true,
        supportsJsonMode: true,
        supportsImageGen: true,
        supportsWebSearch: true,
        imageGenCost: 0.40,
        webSearchCost: 10.00,
        availableProviders: ['openai', 'azure'],
        isRecommended: true,
        priorityRank: 2,
        useCases: ['captions', 'hooks', 'hashtags', 'quick_tasks'],
        description: 'Fastest and cheapest for simple tasks like captions and hashtags',
      },
      {
        modelId: 'openai:gpt-5',
        modelName: 'GPT-5',
        provider: 'openai',
        version: '5',
        contextWindow: 400000,
        maxTokens: 128000,
        latencyMs: 800,
        throughputTps: 103,
        costPer1mInput: 1.25,
        costPer1mOutput: 10.00,
        cacheReadCostPer1m: 0.13,
        costBucket: 'medium',
        capabilities: ['text', 'vision', 'json', 'streaming', 'reasoning'],
        supportsVision: true,
        supportsStreaming: true,
        supportsJsonMode: true,
        supportsFunctionCalling: true,
        supportsImageGen: true,
        supportsWebSearch: true,
        imageGenCost: 10.00,
        webSearchCost: 10.00,
        availableProviders: ['openai'],
        isRecommended: false,
        priorityRank: 5,
        useCases: ['advanced_reasoning', 'complex_tasks'],
        description: 'Advanced reasoning capabilities for complex content strategy',
      },
      {
        modelId: 'openai:gpt-5-mini',
        modelName: 'GPT-5 Mini',
        provider: 'openai',
        version: '5-mini',
        contextWindow: 1000000,
        maxTokens: 66000,
        latencyMs: 300,
        throughputTps: 336,
        costPer1mInput: 0.30,
        costPer1mOutput: 2.50,
        cacheReadCostPer1m: 0.03,
        costBucket: 'low',
        capabilities: ['text', 'json', 'streaming', 'fast'],
        supportsStreaming: true,
        supportsJsonMode: true,
        supportsWebSearch: true,
        webSearchCost: 35.00,
        availableProviders: ['openai'],
        isRecommended: true,
        priorityRank: 3,
        useCases: ['captions', 'hashtags', 'hooks', 'bulk_generation'],
        description: 'Ultra-fast for high-volume simple tasks',
      },

      // Anthropic Claude (High Quality)
      {
        modelId: 'anthropic:claude-sonnet-4.5',
        modelName: 'Claude Sonnet 4.5',
        provider: 'anthropic',
        version: '4.5',
        contextWindow: 1000000,
        maxTokens: 64000,
        latencyMs: 600,
        throughputTps: 69,
        costPer1mInput: 3.00,
        costPer1mOutput: 15.00,
        cacheReadCostPer1m: 0.30,
        cacheWriteCostPer1m: 3.75,
        costBucket: 'high',
        capabilities: ['text', 'vision', 'json', 'reasoning', 'high_quality'],
        supportsVision: true,
        supportsStreaming: true,
        supportsJsonMode: true,
        supportsWebSearch: true,
        webSearchCost: 10.00,
        availableProviders: ['anthropic', 'bedrock', 'vertex'],
        isRecommended: false,
        priorityRank: 10,
        useCases: ['content_strategy', 'brand_voice', 'high_quality_content'],
        description: 'Highest quality for brand-critical content',
      },
      {
        modelId: 'anthropic:claude-3.5-sonnet',
        modelName: 'Claude 3.5 Sonnet',
        provider: 'anthropic',
        version: '3.5',
        contextWindow: 200000,
        maxTokens: 8192,
        latencyMs: 500,
        throughputTps: 61,
        costPer1mInput: 3.00,
        costPer1mOutput: 15.00,
        cacheReadCostPer1m: 0.30,
        cacheWriteCostPer1m: 3.75,
        costBucket: 'high',
        capabilities: ['text', 'vision', 'json', 'reasoning'],
        supportsVision: true,
        supportsStreaming: true,
        supportsJsonMode: true,
        availableProviders: ['anthropic', 'bedrock', 'vertex'],
        isRecommended: false,
        priorityRank: 12,
        useCases: ['creative_writing', 'long_form_content'],
        description: 'Excellent for creative and nuanced content',
      },
      {
        modelId: 'anthropic:claude-3.5-haiku',
        modelName: 'Claude 3.5 Haiku',
        provider: 'anthropic',
        version: '3.5-haiku',
        contextWindow: 200000,
        maxTokens: 8192,
        latencyMs: 300,
        throughputTps: 144,
        costPer1mInput: 0.25,
        costPer1mOutput: 1.25,
        cacheReadCostPer1m: 0.03,
        cacheWriteCostPer1m: 0.30,
        costBucket: 'low',
        capabilities: ['text', 'json', 'streaming', 'fast'],
        supportsStreaming: true,
        supportsJsonMode: true,
        availableProviders: ['anthropic', 'bedrock', 'vertex'],
        isRecommended: true,
        priorityRank: 3,
        useCases: ['captions', 'hooks', 'quick_generation'],
        description: 'Fast and affordable Claude alternative - perfect for captions',
      },

      // Google Gemini (Cost-Effective, Fast)
      {
        modelId: 'google:gemini-2.5-flash',
        modelName: 'Gemini 2.5 Flash',
        provider: 'google',
        version: '2.5-flash',
        contextWindow: 1000000,
        maxTokens: 66000,
        latencyMs: 300,
        throughputTps: 336,
        costPer1mInput: 0.30,
        costPer1mOutput: 2.50,
        cacheReadCostPer1m: 0.03,
        costBucket: 'low',
        capabilities: ['text', 'vision', 'json', 'streaming', 'fast'],
        supportsVision: true,
        supportsStreaming: true,
        supportsJsonMode: true,
        supportsWebSearch: true,
        webSearchCost: 35.00,
        availableProviders: ['google', 'vertex'],
        isRecommended: true,
        priorityRank: 6,
        useCases: ['captions', 'hashtags', 'bulk_generation', 'multimodal'],
        description: 'Excellent speed and cost for multimodal tasks',
      },
      {
        modelId: 'google:gemini-2.5-pro',
        modelName: 'Gemini 2.5 Pro',
        provider: 'google',
        version: '2.5-pro',
        contextWindow: 1000000,
        maxTokens: 66000,
        latencyMs: 1700,
        throughputTps: 242,
        costPer1mInput: 1.25,
        costPer1mOutput: 10.00,
        cacheReadCostPer1m: 0.13,
        costBucket: 'medium',
        capabilities: ['text', 'vision', 'json', 'reasoning', 'long_context'],
        supportsVision: true,
        supportsStreaming: true,
        supportsJsonMode: true,
        supportsWebSearch: true,
        webSearchCost: 35.00,
        availableProviders: ['google', 'vertex'],
        isRecommended: false,
        priorityRank: 8,
        useCases: ['complex_reasoning', 'long_content', 'research'],
        description: 'Pro version with enhanced reasoning',
      },
      {
        modelId: 'google:gemini-3-flash',
        modelName: 'Gemini 3 Flash',
        provider: 'google',
        version: '3-flash',
        contextWindow: 1000000,
        maxTokens: 65000,
        latencyMs: 900,
        throughputTps: 157,
        costPer1mInput: 0.50,
        costPer1mOutput: 3.00,
        cacheReadCostPer1m: 0.05,
        costBucket: 'low',
        capabilities: ['text', 'json', 'streaming', 'fast'],
        supportsStreaming: true,
        supportsJsonMode: true,
        supportsWebSearch: true,
        webSearchCost: 14.00,
        availableProviders: ['google', 'vertex'],
        isRecommended: false,
        priorityRank: 15,
        useCases: ['general_tasks', 'content_generation'],
        description: 'Next-gen Gemini with improved speed',
      },

      // Deepseek (Ultra Cost-Effective)
      {
        modelId: 'deepseek:deepseek-v3.2',
        modelName: 'Deepseek V3.2',
        provider: 'deepseek',
        version: '3.2',
        contextWindow: 164000,
        maxTokens: 66000,
        latencyMs: 1000,
        throughputTps: 33,
        costPer1mInput: 0.26,
        costPer1mOutput: 0.38,
        cacheReadCostPer1m: 0.13,
        costBucket: 'low',
        capabilities: ['text', 'json', 'streaming', 'cost_effective'],
        supportsStreaming: true,
        supportsJsonMode: true,
        availableProviders: ['deepseek', 'deepinfra', 'novita'],
        isRecommended: true,
        priorityRank: 7,
        useCases: ['bulk_generation', 'cost_optimization', 'captions'],
        description: 'Best cost-performance ratio for bulk tasks',
      },
      {
        modelId: 'deepseek:deepseek-r1',
        modelName: 'Deepseek R1',
        provider: 'deepseek',
        version: 'r1',
        contextWindow: 128000,
        maxTokens: 64000,
        latencyMs: 1300,
        throughputTps: 35,
        costPer1mInput: 0.28,
        costPer1mOutput: 0.42,
        costBucket: 'low',
        capabilities: ['text', 'reasoning', 'json'],
        supportsJsonMode: true,
        supportsStreaming: true,
        availableProviders: ['deepseek'],
        isRecommended: false,
        priorityRank: 20,
        useCases: ['reasoning', 'analysis'],
        description: 'Reasoning-optimized variant',
      },

      // Meta Llama (Open Source)
      {
        modelId: 'meta:llama-3.3-70b',
        modelName: 'Llama 3.3 70B',
        provider: 'meta',
        version: '3.3-70b',
        contextWindow: 131000,
        maxTokens: 66000,
        latencyMs: 1300,
        throughputTps: 285,
        costPer1mInput: 0.15,
        costPer1mOutput: 1.50,
        costBucket: 'low',
        capabilities: ['text', 'json', 'streaming', 'open_source'],
        supportsStreaming: true,
        supportsJsonMode: true,
        availableProviders: ['meta', 'alibaba', 'novita'],
        isRecommended: false,
        priorityRank: 18,
        useCases: ['general_content', 'cost_optimization'],
        description: 'Open-source alternative with good performance',
      },

      // xAI Grok (Fast, Competitive)
      {
        modelId: 'xai:grok-3',
        modelName: 'Grok 3',
        provider: 'xai',
        version: '3',
        contextWindow: 131000,
        maxTokens: 131000,
        latencyMs: 600,
        throughputTps: 66,
        costPer1mInput: 5.00,
        costPer1mOutput: 25.00,
        costBucket: 'high',
        capabilities: ['text', 'json', 'streaming', 'reasoning'],
        supportsStreaming: true,
        supportsJsonMode: true,
        availableProviders: ['xai'],
        isRecommended: false,
        priorityRank: 25,
        useCases: ['creative_content', 'conversation'],
        description: 'xAI flagship model with personality',
      },
      {
        modelId: 'xai:grok-3-fast',
        modelName: 'Grok 3 Fast',
        provider: 'xai',
        version: '3-fast',
        contextWindow: 256000,
        maxTokens: 256000,
        latencyMs: 900,
        throughputTps: 81,
        costPer1mInput: 0.22,
        costPer1mOutput: 0.88,
        costBucket: 'low',
        capabilities: ['text', 'json', 'streaming', 'fast'],
        supportsStreaming: true,
        supportsJsonMode: true,
        availableProviders: ['xai'],
        isRecommended: false,
        priorityRank: 22,
        useCases: ['quick_content', 'real_time'],
        description: 'Optimized for speed',
      },

      // Alibaba Qwen (Multilingual, Cost-Effective)
      {
        modelId: 'alibaba:qwen3-max',
        modelName: 'Qwen3 Max',
        provider: 'alibaba',
        version: '3-max',
        contextWindow: 1000000,
        maxTokens: 66000,
        latencyMs: 1700,
        throughputTps: 54,
        costPer1mInput: 1.00,
        costPer1mOutput: 5.00,
        cacheReadCostPer1m: 0.20,
        costBucket: 'medium',
        capabilities: ['text', 'json', 'multilingual', 'reasoning'],
        supportsStreaming: true,
        supportsJsonMode: true,
        availableProviders: ['alibaba'],
        isRecommended: false,
        priorityRank: 30,
        useCases: ['multilingual_content', 'asian_markets'],
        description: 'Excellent for multilingual and Asian market content',
      },

      // Perplexity (Web Search Integrated)
      {
        modelId: 'perplexity:sonar',
        modelName: 'Sonar',
        provider: 'perplexity',
        version: 'sonar',
        contextWindow: 127000,
        maxTokens: 8192,
        latencyMs: 1500,
        throughputTps: 93,
        costPer1mInput: 1.00,
        costPer1mOutput: 1.00,
        costBucket: 'low',
        capabilities: ['text', 'web_search', 'real_time'],
        supportsStreaming: true,
        supportsWebSearch: true,
        availableProviders: ['perplexity'],
        isRecommended: false,
        priorityRank: 35,
        useCases: ['trending_research', 'real_time_data'],
        description: 'Real-time web search for trending topics',
      },

      // ========== IMAGE GENERATION MODELS ==========
      
      {
        modelId: 'bfl:flux-2-pro',
        modelName: 'FLUX 2 Pro',
        provider: 'bfl',
        version: '2-pro',
        contextWindow: 512,
        maxTokens: 0,
        latencyMs: 0,
        throughputTps: 0,
        imageGenCost: 0.08,
        costBucket: 'medium',
        capabilities: ['image_generation', 'high_quality'],
        supportsImageGen: true,
        availableProviders: ['bfl'],
        isRecommended: true,
        priorityRank: 1,
        useCases: ['social_media_images', 'marketing_visuals'],
        description: 'Best quality for social media image generation',
      },
      {
        modelId: 'bfl:flux-2-max',
        modelName: 'FLUX 2 Max',
        provider: 'bfl',
        version: '2-max',
        contextWindow: 67000,
        maxTokens: 67000,
        imageGenCost: 0.07,
        costBucket: 'medium',
        capabilities: ['image_generation', 'multimodal'],
        supportsImageGen: true,
        supportsVideoGen: true,
        videoGenCost: 0.07,
        availableProviders: ['bfl'],
        isRecommended: true,
        priorityRank: 2,
        useCases: ['social_posts', 'video_generation'],
        description: 'Multimodal generation for video and images',
      },
      {
        modelId: 'bfl:flux-pro-1.1',
        modelName: 'FLUX Pro 1.1',
        provider: 'bfl',
        version: '1.1',
        imageGenCost: 0.04,
        costBucket: 'low',
        capabilities: ['image_generation'],
        supportsImageGen: true,
        availableProviders: ['bfl', 'prodia'],
        isRecommended: true,
        priorityRank: 3,
        useCases: ['quick_images', 'bulk_generation'],
        description: 'Fast and affordable image generation',
      },
      {
        modelId: 'google:imagen-4.0-fast-generate-001',
        modelName: 'Imagen 4.0 Fast',
        provider: 'google',
        version: '4.0-fast',
        contextWindow: 33000,
        maxTokens: 66000,
        latencyMs: 400,
        throughputTps: 205,
        imageGenCost: 0.04,
        costBucket: 'low',
        capabilities: ['text', 'image_generation', 'fast'],
        supportsImageGen: true,
        supportsVision: true,
        availableProviders: ['google', 'vertex'],
        isRecommended: true,
        priorityRank: 4,
        useCases: ['social_images', 'quick_visuals'],
        description: 'Fast Google image generation',
      },
      {
        modelId: 'recraft:recraft-v3',
        modelName: 'Recraft V3',
        provider: 'recraft',
        version: 'v3',
        imageGenCost: 0.04,
        costBucket: 'low',
        capabilities: ['image_generation', 'style_control'],
        supportsImageGen: true,
        availableProviders: ['recraft'],
        isRecommended: false,
        priorityRank: 10,
        useCases: ['styled_images', 'brand_visuals'],
        description: 'Style-controlled image generation',
      },

      // ========== SPECIALIZED MODELS ==========
      
      // Mistral (Code & Fast)
      {
        modelId: 'mistral:mistral-large-3',
        modelName: 'Mistral Large 3',
        provider: 'mistral',
        version: 'large-3',
        contextWindow: 256000,
        maxTokens: 256000,
        latencyMs: 1200,
        throughputTps: 46,
        costPer1mInput: 0.50,
        costPer1mOutput: 1.50,
        costBucket: 'low',
        capabilities: ['text', 'json', 'streaming', 'multilingual'],
        supportsStreaming: true,
        supportsJsonMode: true,
        availableProviders: ['mistral'],
        isRecommended: false,
        priorityRank: 28,
        useCases: ['general_content', 'multilingual'],
        description: 'Strong multilingual capabilities',
      },

      // Deepinfra providers (aggregator)
      {
        modelId: 'meta:llama-3.1-70b',
        modelName: 'Llama 3.1 70B',
        provider: 'meta',
        version: '3.1-70b',
        contextWindow: 131000,
        maxTokens: 16000,
        latencyMs: 400,
        throughputTps: 154,
        costPer1mInput: 1.35,
        costPer1mOutput: 5.40,
        cacheReadCostPer1m: 0.40,
        costBucket: 'medium',
        capabilities: ['text', 'json', 'streaming'],
        supportsStreaming: true,
        supportsJsonMode: true,
        availableProviders: ['bedrock', 'deepinfra'],
        isRecommended: false,
        priorityRank: 24,
        useCases: ['general_content', 'open_source'],
        description: 'Popular open-source model',
      },
    ];

    // Insert models
    for (const model of models) {
      await queryRunner.query(
        `
        INSERT INTO "ai_models" (
          "model_id", "model_name", "provider", "version", "context_window", "max_tokens",
          "latency_ms", "throughput_tps", "cost_per_1m_input", "cost_per_1m_output",
          "cache_read_cost_per_1m", "cache_write_cost_per_1m", "image_gen_cost", "video_gen_cost",
          "web_search_cost", "cost_bucket", "capabilities", "supports_vision", "supports_streaming",
          "supports_json_mode", "supports_function_calling", "supports_image_gen", "supports_video_gen",
          "supports_web_search", "available_providers", "is_recommended", "priority_rank", "use_cases",
          "description", "is_active", "metadata"
        ) VALUES (
          $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20,
          $21, $22, $23, $24, $25, $26, $27, $28, $29, $30, $31
        ) ON CONFLICT (model_id) DO UPDATE SET
          model_name = EXCLUDED.model_name,
          context_window = EXCLUDED.context_window,
          max_tokens = EXCLUDED.max_tokens,
          latency_ms = EXCLUDED.latency_ms,
          throughput_tps = EXCLUDED.throughput_tps,
          cost_per_1m_input = EXCLUDED.cost_per_1m_input,
          cost_per_1m_output = EXCLUDED.cost_per_1m_output,
          updated_at = CURRENT_TIMESTAMP
        `,
        [
          model.modelId,
          model.modelName,
          model.provider,
          model.version || null,
          model.contextWindow || null,
          model.maxTokens || null,
          model.latencyMs || null,
          model.throughputTps || null,
          model.costPer1mInput || null,
          model.costPer1mOutput || null,
          model.cacheReadCostPer1m || null,
          model.cacheWriteCostPer1m || null,
          model.imageGenCost || null,
          model.videoGenCost || null,
          model.webSearchCost || null,
          model.costBucket,
          model.capabilities || [],
          model.supportsVision || false,
          model.supportsStreaming || false,
          model.supportsJsonMode || false,
          model.supportsFunctionCalling || false,
          model.supportsImageGen || false,
          model.supportsVideoGen || false,
          model.supportsWebSearch || false,
          model.availableProviders || [],
          model.isRecommended || false,
          model.priorityRank || 999,
          model.useCases || [],
          model.description || '',
          true,
          {},
        ],
      );
    }

    // Seed task categories
    const categories = [
      {
        categoryKey: 'content_ideas',
        categoryName: 'Content Ideas Generation',
        description: 'Generate 5 content storylines and ideas',
        requiredCapabilities: ['text', 'reasoning', 'json'],
        preferredCapabilities: ['high_quality', 'creativity'],
        defaultPriority: 'high',
        defaultComplexity: 'high',
        typicalMaxTokens: 2000,
        typicalTemperature: 0.90,
        tags: ['creative', 'strategic', 'complex'],
      },
      {
        categoryKey: 'captions',
        categoryName: 'Caption Generation',
        description: 'Generate social media captions',
        requiredCapabilities: ['text', 'json'],
        preferredCapabilities: ['fast', 'cost_effective'],
        defaultPriority: 'normal',
        defaultComplexity: 'low',
        typicalMaxTokens: 500,
        typicalTemperature: 0.80,
        tags: ['fast', 'simple', 'bulk'],
      },
      {
        categoryKey: 'captions_with_media',
        categoryName: 'Caption from Image/Video',
        description: 'Generate captions based on visual content',
        requiredCapabilities: ['text', 'vision', 'json'],
        preferredCapabilities: ['multimodal'],
        defaultPriority: 'normal',
        defaultComplexity: 'medium',
        typicalMaxTokens: 600,
        typicalTemperature: 0.75,
        tags: ['vision', 'multimodal'],
      },
      {
        categoryKey: 'hooks',
        categoryName: 'Hooks Generation',
        description: 'Generate attention-grabbing hooks',
        requiredCapabilities: ['text', 'json'],
        preferredCapabilities: ['fast', 'creative'],
        defaultPriority: 'normal',
        defaultComplexity: 'low',
        typicalMaxTokens: 300,
        typicalTemperature: 0.95,
        tags: ['creative', 'fast'],
      },
      {
        categoryKey: 'hashtags',
        categoryName: 'Hashtag Generation',
        description: 'Generate SEO-optimized hashtags',
        requiredCapabilities: ['text', 'json'],
        preferredCapabilities: ['fast', 'cost_effective'],
        defaultPriority: 'low',
        defaultComplexity: 'low',
        typicalMaxTokens: 200,
        typicalTemperature: 0.70,
        tags: ['seo', 'fast', 'simple'],
      },
      {
        categoryKey: 'image_generation',
        categoryName: 'Image Generation',
        description: 'Generate images for social media',
        requiredCapabilities: ['image_generation'],
        preferredCapabilities: ['high_quality', 'fast'],
        defaultPriority: 'normal',
        defaultComplexity: 'medium',
        typicalMaxTokens: 0,
        typicalTemperature: 0.80,
        tags: ['visual', 'creative'],
      },
      {
        categoryKey: 'trending_research',
        categoryName: 'Trending Topics Research',
        description: 'Research trending topics and hashtags',
        requiredCapabilities: ['text', 'web_search'],
        preferredCapabilities: ['real_time'],
        defaultPriority: 'normal',
        defaultComplexity: 'medium',
        typicalMaxTokens: 1000,
        typicalTemperature: 0.60,
        tags: ['research', 'trends', 'real_time'],
      },
    ];

    for (const category of categories) {
      await queryRunner.query(
        `
        INSERT INTO "ai_task_categories" (
          "category_key", "category_name", "description", "required_capabilities",
          "preferred_capabilities", "default_priority", "default_complexity",
          "typical_max_tokens", "typical_temperature", "tags", "is_active"
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
        ON CONFLICT (category_key) DO UPDATE SET
          category_name = EXCLUDED.category_name,
          description = EXCLUDED.description,
          updated_at = CURRENT_TIMESTAMP
        `,
        [
          category.categoryKey,
          category.categoryName,
          category.description,
          category.requiredCapabilities,
          category.preferredCapabilities,
          category.defaultPriority,
          category.defaultComplexity,
          category.typicalMaxTokens,
          category.typicalTemperature,
          category.tags,
          true,
        ],
      );
    }
  }

  public async down(queryRunner: QueryRunner): Promise<void> {
    // Remove seeded data
    await queryRunner.query(`DELETE FROM "ai_models"`);
    await queryRunner.query(`DELETE FROM "ai_task_categories"`);
    
    // Remove new columns
    await queryRunner.query(`
      ALTER TABLE "ai_models"
      DROP COLUMN IF EXISTS "latency_ms",
      DROP COLUMN IF EXISTS "throughput_tps",
      DROP COLUMN IF EXISTS "cost_per_1m_input",
      DROP COLUMN IF EXISTS "cost_per_1m_output",
      DROP COLUMN IF EXISTS "cache_read_cost_per_1m",
      DROP COLUMN IF EXISTS "cache_write_cost_per_1m",
      DROP COLUMN IF EXISTS "image_gen_cost",
      DROP COLUMN IF EXISTS "video_gen_cost",
      DROP COLUMN IF EXISTS "web_search_cost",
      DROP COLUMN IF EXISTS "supports_image_gen",
      DROP COLUMN IF EXISTS "supports_video_gen",
      DROP COLUMN IF EXISTS "supports_web_search",
      DROP COLUMN IF EXISTS "available_providers";
    `);
  }
}
